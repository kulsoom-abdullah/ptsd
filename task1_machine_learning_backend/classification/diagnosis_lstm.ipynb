{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "import glob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2745\n",
      "3293\n",
      "2787\n",
      "Total Clients processed:  3\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/shikha/Documents/projects/omdena/dataset/example/*'\n",
    "files = glob.glob(path)\n",
    "data = {}\n",
    "# label 0 means no ptsd\n",
    "data[0] = []\n",
    "# label 1 means ptsd\n",
    "data[1] = []\n",
    "count = 0\n",
    "separator = \"\"\n",
    "for name in files: # 'file' is a builtin type, 'name' is a less-ambiguous variable name.\n",
    "    try:\n",
    "        with open(name) as f:\n",
    "            content = f.readlines()\n",
    "            print len(content)\n",
    "            # limiting the conversation for now\n",
    "            content = separator.join(content[:100])\n",
    "            count += 1\n",
    "            if count % 2 == 0:\n",
    "                data[0].append(content)\n",
    "            else:\n",
    "                data[1].append(content)\n",
    "    except IOError as exc:\n",
    "        print \"IO Error\"\n",
    "\n",
    "print \"Total Clients processed: \", count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(gloveFile):\n",
    "    print \"\\nLoading Glove Model...\"\n",
    "    f = open(gloveFile,'r')\n",
    "    wordList = []\n",
    "    wordVectors = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        wordList.append(word)\n",
    "        wordVectors.append(embedding)\n",
    "    wordVectors = np.array(wordVectors)\n",
    "    print \"Done. \",len(wordList),\" words loaded!\"\n",
    "    print wordVectors[wordList.index('woman')]\n",
    "\n",
    "    return wordList, wordVectors\n",
    "\n",
    "def load_dataset(filename=\"\"):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for label, transcripts in data.items():\n",
    "        X += [val for val in transcripts]\n",
    "        Y += [label] * len(transcripts)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Glove Model...\n",
      "Done.  400000  words loaded!\n",
      "[ 0.59368   0.44825   0.5932    0.074134  0.11141   1.2793    0.16656\n",
      "  0.2407    0.39045   0.32766  -0.75034   0.35007   0.76057   0.38067\n",
      "  0.17517   0.031791  0.46849  -0.21653  -0.46282   0.39967   0.16623\n",
      " -0.011477  0.044059  0.30325   0.6153    0.47047  -0.44036  -1.5963\n",
      "  0.18433   0.23193   0.20452   0.51617   0.65734  -0.3452    0.23446\n",
      " -0.62004  -0.68741   0.28575   1.0605    0.46916  -0.85149   0.10154\n",
      "  0.21426  -0.20587   0.23636   0.21321  -0.21287   0.12107   0.18766\n",
      " -0.23282  -0.25499  -0.39631   0.84379   1.6801   -0.40941  -1.9976\n",
      " -0.69868   0.21732   1.2197    0.55126   0.44095   0.72588  -0.092053\n",
      " -0.022406  0.72039   0.1076    0.84116   0.30312  -0.42544   0.056362\n",
      "  0.13109  -0.071181 -0.10579   0.56677   0.54547   0.84113   0.14861\n",
      " -0.62628  -0.68391  -1.0831   -0.088385  0.32167   0.47794   0.091868\n",
      " -1.2559   -1.2268    0.085401  0.36833   0.081566 -0.76611   0.87751\n",
      " -0.22008   0.82401  -0.092207 -0.45941   0.46571  -0.56018  -0.54648\n",
      "  0.15162  -0.30754 ]\n"
     ]
    }
   ],
   "source": [
    "root = \"/Users/shikha/Documents/projects/omdena/\"\n",
    "glove_file = root + \"glove/glove.6B.100d.txt\"\n",
    "\n",
    "# Load glove file\n",
    "wordsList, wordVectors = load_glove_model(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = dataset + \"/train/abc\"\n",
    "X, Y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, decode=True):\n",
    "    if decode:\n",
    "        return word_tokenize(doc.decode('utf-8'))\n",
    "    else:\n",
    "        return word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average no of words in a sentence =  3210\n"
     ]
    }
   ],
   "source": [
    "# counting average no words in a sentence\n",
    "count = 0\n",
    "for transcript in X:\n",
    "    words = tokenize(transcript)\n",
    "    count += len(words)\n",
    "avg = count/X.shape[0]\n",
    "print \"Average no of words in a sentence = \", avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count       3.000000\n",
       "mean     3210.333333\n",
       "std       646.973209\n",
       "min      2501.000000\n",
       "25%      2931.500000\n",
       "50%      3362.000000\n",
       "75%      3565.000000\n",
       "max      3768.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "transcripts_len = [len(tokenize(transcript)) for transcript in X]\n",
    "pd.Series(transcripts_len).hist()\n",
    "plt.show()\n",
    "pd.Series(transcripts_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually based on average no of words in the sentence\n",
    "maxSeqLength = 25\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once and save data in npy file\n",
    "# converting words to ids from glove\n",
    "\n",
    "# ids = np.zeros((X.shape[0], maxSeqLength), dtype='int32')\n",
    "# i = 0\n",
    "# for transcript in X:\n",
    "#     words = tokenize(transcript)\n",
    "#     indexCounter = 0\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             ids[i][indexCounter] = wordsList.index(word)\n",
    "#         except ValueError:\n",
    "#             ids[i][indexCounter] = 399999 #Vector for unknown words\n",
    "#         indexCounter = indexCounter + 1\n",
    "#         if indexCounter >= maxSeqLength:\n",
    "#            break\n",
    "#     i += 1\n",
    "\n",
    "# np.save('trascript_ids', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('trascript_ids.npy')\n",
    "\n",
    "# train_len = int(0.8 * X.shape[0])\n",
    "# X_train = X[0:train_len]\n",
    "# Y_train = Y[0:train_len]\n",
    "# X_test = X[train_len:]\n",
    "# Y_test = Y[train_len:]\n",
    "train_len = X.shape[0]\n",
    "X_train = X\n",
    "Y_train = Y\n",
    "X_test = X\n",
    "Y_test = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train))\n",
    "# valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test))\n",
    "# dataloaders\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sample input size: ', torch.Size([1, 25]))\n",
      "('Sample input: \\n', tensor([[399999, 399999, 399999, 399999,     45, 399999, 399999,     45, 399999,\n",
      "             45,     23, 399999,   9085,     24, 399999,      1,    886,      9,\n",
      "            253,      2, 399999,    269,     61,     53,    809]],\n",
      "       dtype=torch.int32))\n",
      "()\n",
      "('Sample label size: ', torch.Size([1]))\n",
      "('Sample label: \\n', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class diagnosis_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers, num_outputs, vocab_len, hidden_size):\n",
    "        super(diagnosis_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_dim)\n",
    "        print 'Embedding layer is ', self.embeddings\n",
    "        print 'Embedding layer weights ', self.embeddings.weight.shape\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=num_outputs)\n",
    "#         self.output = nn.LogSoftmax(dim=1)\n",
    "        self.output = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input, batch_size):\n",
    "        embeddings_out = self.embeddings(input)\n",
    "        print 'Embedding layer output shape', embeddings_out.shape\n",
    "#         print 'Embedding layer output ', embeddings_out\n",
    "        \n",
    "        # initializing the hidden state to 0\n",
    "        hidden=None\n",
    "        lstm_out, h = self.lstm(embeddings_out, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_size)\n",
    "        lstm_out = torch.sum(lstm_out, dim=0)\n",
    "        print 'LSTM layer output shape', lstm_out.shape\n",
    "#         print 'LSTM layer output ', lstm_out\n",
    "        \n",
    "        fc_out = self.fc(lstm_out.contiguous().view(-1, self.hidden_size))\n",
    "        print 'FC layer output shape', fc_out.shape\n",
    "#         print 'FC layer output ', fc_out\n",
    "        \n",
    "#         log_probs = F.log_softmax(fc_out)\n",
    "#         print \"log probs shape\", log_probs.shape\n",
    "#         return log_probs\n",
    "        sigm_out = self.output(fc_out)\n",
    "        print 'Sigmoid layer output shape', sigm_out.shape\n",
    "        print 'Sigmoid layer output ', sigm_out\n",
    "        \n",
    "        out = sigm_out.view(batch_size, -1)\n",
    "        print 'Output layer output shape', out.shape\n",
    "        print 'Output layer output ', out\n",
    "        \n",
    "        print 'Final ptsd prediction, ', out[:,-1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer is  Embedding(400001, 100)\n",
      "Embedding layer weights  torch.Size([400001, 100])\n",
      "diagnosis_LSTM(\n",
      "  (embeddings): Embedding(400001, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "num_layers = 2\n",
    "num_classes = 1\n",
    "hidden_size = 256\n",
    "embedding_dim = 100 # it should match with the glove file embeddings dimension above\n",
    "vocab_len = len(wordsList)+1 # +1 for the 0 padding\n",
    "\n",
    "net = diagnosis_LSTM(embedding_dim, num_layers, num_classes, vocab_len, hidden_size)\n",
    "print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_x shape torch.Size([1, 25])\n",
      "Embedding layer output shape torch.Size([1, 25, 100])\n",
      "LSTM layer output shape torch.Size([256])\n",
      "FC layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output  tensor([[0.4550]], grad_fn=<SigmoidBackward>)\n",
      "Output layer output shape torch.Size([1, 1])\n",
      "Output layer output  tensor([[0.4550]], grad_fn=<ViewBackward>)\n",
      "Final sentiment prediction,  tensor([0.4550], grad_fn=<SelectBackward>)\n",
      "(0, tensor(0., grad_fn=<AddBackward>))\n",
      "sample_x shape torch.Size([1, 25])\n",
      "Embedding layer output shape torch.Size([1, 25, 100])\n",
      "LSTM layer output shape torch.Size([256])\n",
      "FC layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output  tensor([[0.4550]], grad_fn=<SigmoidBackward>)\n",
      "Output layer output shape torch.Size([1, 1])\n",
      "Output layer output  tensor([[0.4550]], grad_fn=<ViewBackward>)\n",
      "Final sentiment prediction,  tensor([0.4550], grad_fn=<SelectBackward>)\n",
      "(1, tensor(0., grad_fn=<AddBackward>))\n",
      "sample_x shape torch.Size([1, 25])\n",
      "Embedding layer output shape torch.Size([1, 25, 100])\n",
      "LSTM layer output shape torch.Size([256])\n",
      "FC layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output shape torch.Size([1, 1])\n",
      "Sigmoid layer output  tensor([[0.4550]], grad_fn=<SigmoidBackward>)\n",
      "Output layer output shape torch.Size([1, 1])\n",
      "Output layer output  tensor([[0.4550]], grad_fn=<ViewBackward>)\n",
      "Final sentiment prediction,  tensor([0.4550], grad_fn=<SelectBackward>)\n",
      "(2, tensor(0., grad_fn=<AddBackward>))\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.NLLLoss\n",
    "optim = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    ep_loss = 0.\n",
    "        \n",
    "    # loop over training data\n",
    "    sample_x = sample_x.type(torch.LongTensor)\n",
    "    print \"sample_x shape\", sample_x.shape  # should be (batch_size, max_seq_length)\n",
    "    for i in range(len(sample_x)):\n",
    "        sent = torch.LongTensor(sample_x[i]).unsqueeze(0)\n",
    "        target = sample_y[i].unsqueeze(0)\n",
    "        \n",
    "        pred = net.forward(sent, batch_size)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        ep_loss += loss\n",
    "    \n",
    "    print(epoch, ep_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
